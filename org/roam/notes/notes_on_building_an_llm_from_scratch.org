:lPROPERTIES:
:ID:       F29CBBCE-BF32-4A7F-A576-A3DA674F540A
:END:
#+title: Notes on building an LLM from scratch
#+filetags: :notes:
#+hugo_section: notes
#+hugo_front_matter_format: yaml
#+date: [2025-05-21 Wed]
#+hugo_lastmod: [2025-11-26 Wed]
*Connections*: [[id:DAC08F09-73CE-4603-8902-147EAF49B103][AI]]  

* Chapter 1: Understanding Large Language Models
 input text -> tokenize text -> token ids -> token embeddings -> GPT decoder only transformer -> postprocessing steps -> output 

- The "large" in large language models refers to the number of parameters
- The first step in creating an LLM is to train it on a large corpus of text data, sometimes referred to as "raw" text
** LLM pretraining does not require labeled data, and instead uses
   self-supervised learning where the model generates its own labels.
   The "labels" come frome the structure of the data itself
** Transformer Architecture
*** Originally developed for machine translation  
*** Consists of two submodules, an encoder and decoder 
*** Both submodules consist of many layers connected by a self-attention mechanism 
**** The self-attention mechanism is a key component which allows the model to weigh the importance of words or tokens in a sequence relative to each other 
*** Systems like BERT focus on the encoder, while systems like GPT focus on the decoder 
*** GPT is considered an "autoregressive" model, meaning it uses previous outputs as inputs for future predictions 
** Three main stages of coding an LLM 
*** Implementing the LLM architecture and data preperation process 
*** Pretraining an LLM to create a foundation model 
*** Fine-tuning the foundation model to become a personal assistant or text classifier 

* Chapter 2: Working with text data 
** Data preperation and sampling
*** Word embeddings 
**** Deep neural networks and LLMs cannot process text directly, so we need to represent words as continuous-valued vectors 
**** Converting data into vectors is often referred to as embedding 
**** There are word, sentence, paragraph, and whole document embeddings. Sentence and paragraph are useful for RAG 
**** Forget 3 dimensions, systems like GPT-3 use 12,288 dimensions. What does that even mean? 
*** Tokenizing Text 
**** <|unk|> tokens are used to indicate words that aren't in the vocab 
**** <|endoftext|> tokens are used to indicate the beginning of a new text source when multiple sources are used
**** BPE or "Byte Pair Encoding" is a commonly used, sophisticated tokenization scheme that can be installed with the [[https://github.com/openai/tiktoken][tiktoken]] package  
*** Data Sampling With a Sliding Window
**** [LLMs] _learn_ to predict one word at a time 
**** [LLMs learn] _to_ predict one word at a time 
**** [LLMs learn to] _predict_ one word at a time 
***** Given a text sample, the LLM prediction task during training is to predict the next word that follows the input block.
In the example above, the input blocks are in [brackets] and the
target word is _underlined_
** Creating Token Embeddings
*** The last step in preparing the input text for an LLM is to convert the token IDs into embedding vectors 
**** Step 1: Initialize the vectors with random weights 
***** A continuous vector representation is necessary because LLMs like GPT are deep neural networks trained with the backpropagation algorithm. 
** Summary
*** LLMs require textual data to be converted into numerical representations known as vectors, since they can't process raw text. 
*** Embeddings transform discrete data like words or images into continuous vector spaces, making them compatible with NNs 
*** As the first step, raw text is broken into tokens, which can be words or characters. Then, the tokens are converted into integer representations, termed token IDs 
*** Special tokens such as <|unk|> and <|endoftext|> can be added to enhance the model's understanding
*** The byte pair encoding (BPE) tokenizer used for LLMs like GPT2 and 3 can efficiently handle unknown words by breaking them down into subword units or characters
*** We use a sliding window approach on the tokenized data to generate input-target pairs for LLM training.
*** Embedding layers in PyTorch function as a lookup operation, retrieving vectors corresponding to token IDs. The resulting embedding vectors provide continuous representations of tokens 
*** Absolute or relative positional embeddings are created to augment the LLMs lack of ability to understand a token's position in a sequence. OpenAIs models use absolute positional embeddings.

* Coding Attention Mechanisms 
